{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VThVF6zPEHY"
      },
      "source": [
        "# 1. Crawler\n",
        "\n",
        "## 1.0. Related example\n",
        "\n",
        "This code shows `wget`-like tool written in python. Run it from console (`python wget.py`), make it work. Check the code, reuse, and modify for your needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2t2DtzQ9PEHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2922ddd8-22c3-411f-da60-97f88bd4a029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content-Encoding: gzip\n",
            "Content-Type: text/plain; charset=utf-8\n",
            "x-fb-rlafr: 0\n",
            "document-policy: force-load-at-top\n",
            "Pragma: no-cache\n",
            "Cache-Control: private, no-cache, no-store, must-revalidate\n",
            "Expires: Tue, 08 Feb 2022 03:14:50 +0000\n",
            "X-Content-Type-Options: nosniff\n",
            "X-XSS-Protection: 0\n",
            "X-Frame-Options: DENY\n",
            "Access-Control-Expose-Headers: X-FB-Debug, X-Loader-Length\n",
            "Access-Control-Allow-Methods: OPTIONS\n",
            "Access-Control-Allow-Credentials: true\n",
            "Access-Control-Allow-Origin: https://facebook.com\n",
            "Vary: Origin, Accept-Encoding\n",
            "Strict-Transport-Security: max-age=15552000; preload\n",
            "X-FB-Debug: mugnxkUcNdInR4+qw2i7MaIm+otb+V9DLWxOqRPKfaYQ0RLYPbh3+vXJf4LVPqVy3pXv8cVdbEjErR1oqMTExw==\n",
            "Date: Mon, 07 Feb 2022 03:14:51 GMT\n",
            "Priority: u=3,i\n",
            "Alt-Svc: h3=\":443\"; ma=3600, h3-29=\":443\"; ma=3600\n",
            "Transfer-Encoding: chunked\n",
            "Connection: keep-alive\n",
            "\n",
            "File saved as robots.txt\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "\n",
        "\n",
        "def wget(url, filename):\n",
        "    # allow redirects - in case file is relocated\n",
        "    resp = requests.get(url, allow_redirects=True)\n",
        "    # this can also be 2xx, but for simplicity now we stick to 200\n",
        "    # you can also check for `resp.ok`\n",
        "    if resp.status_code != 200:\n",
        "        print(resp.status_code, resp.reason, 'for', url)\n",
        "        return\n",
        "    \n",
        "    # just to be cool and print something\n",
        "    print(*[f\"{key}: {value}\" for key, value in resp.headers.items()], sep='\\n')\n",
        "    print()\n",
        "    \n",
        "    # try to extract filename from url\n",
        "    if filename is None:\n",
        "        # start with http*, ends if ? or # appears (or none of)\n",
        "        m = re.search(\"^http.*/([^/\\?#]*)[\\?#]?\", url)\n",
        "        filename = m.group(1)\n",
        "        if not filename:\n",
        "            raise NameError(f\"Filename neither given, nor found for {url}\")\n",
        "\n",
        "    # what will you do in case 2 websites store file with the same name?\n",
        "    if os.path.exists(filename):\n",
        "        raise OSError(f\"File {filename} already exists\")\n",
        "    \n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(resp.content)\n",
        "        print(f\"File saved as {filename}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description='download file.')\n",
        "    parser.add_argument(\"-O\", type=str, default=None, dest='filename', help=\"output file name. Default -- taken from resource\")\n",
        "    parser.add_argument(\"url\", type=str, default=None, help=\"Provide URL here\")\n",
        "    args = parser.parse_args('https://facebook.com/robots.txt'.split() )\n",
        "    wget(args.url, args.filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcyWzZB5PEHf"
      },
      "source": [
        "### 1.0.1. How to parse a page?\n",
        "\n",
        "If you build a crawler, you might follow one of the approaches:\n",
        "1. search for URLs in the page, assuming this is just a text.\n",
        "2. search for URLs in the places where URLs should appear: `<a href=..`, `<img src=...`, `<iframe src=...` and so on.\n",
        "\n",
        "To follow the first approach you can rely on some good regular expression. [Like this](https://stackoverflow.com/a/3809435).\n",
        "\n",
        "To follow the second approach just read one of these: [short answer](https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup) or [exhaustive explanation](https://hackersandslackers.com/scraping-urls-with-beautifulsoup/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7fkdVU1PEHg"
      },
      "source": [
        "## 1.1. [15] Download and persist #\n",
        "Please complete a code for `load()`, `download()` and `persist()` methods of `Document` class. What they do:\n",
        "- for a given URL `download()` method downloads binary data and stores in `self.content`. It returns `True` for success, else `False`.\n",
        "- `persist()` method saves `self.content` somewhere in file system. We do it to avoid multiple downloads (for caching in other words).\n",
        "- `load()` method loads data from hard drive. Returns `True` for success.\n",
        "\n",
        "Tests checks that your code somehow works.\n",
        "\n",
        "**NB Passing the test doesn't mean you correctly completed the task.** These are **criteria, which have to be fullfilled**:\n",
        "1. URL is a unique identifier (as it is a subset of URI). Thus, documents with different URLs should be stored in different files. Typical errors: documents from the same domain are overwritten to the same file, URLs with similar endings are downloaded to the same file, etc.\n",
        "2. The document can be not only a text file, but also a binary. Pay attention that if you download `mp3` file, it still can be played. Hint: don't hurry to convert everything to text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ebe-PCLrPEHh"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from urllib.parse import quote\n",
        "\n",
        "class Document:\n",
        "    \n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        \n",
        "    def get(self):\n",
        "        if not self.load():\n",
        "            if not self.download():\n",
        "                raise FileNotFoundError(self.url)\n",
        "            else:\n",
        "                self.persist()\n",
        "    \n",
        "    def download(self):\n",
        "        # downloading self.url content, store it in self.content and return True in case of success\n",
        "        try:\n",
        "            self.content = requests.get(self.url, allow_redirects=True).content\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "    \n",
        "    def persist(self):\n",
        "        # writing document content to hard drive\n",
        "        # url is attached with a file name for uniqueness\n",
        "        url = quote(self.url, safe = \"\")\n",
        "        open(\"nfile_\" + url, 'wb').write(self.content)\n",
        "            \n",
        "    def load(self):\n",
        "        # loading content from hard drive, store it in self.content and return True in case of success\n",
        "        try:\n",
        "            url = quote(self.url, safe = \"\")\n",
        "            f = open(\"nfile_\" + url, \"rb\")\n",
        "            self.content = f.read()\n",
        "            f.close()\n",
        "            return True\n",
        "        except:\n",
        "            return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu6KRjRCPEHi"
      },
      "source": [
        "### 1.1.1. Tests ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "58sLQOvjPEHi"
      },
      "outputs": [],
      "source": [
        "doc = Document('http://sprotasov.ru/data/iu.txt')\n",
        "\n",
        "doc.get()\n",
        "assert doc.content, \"Document download failed\"\n",
        "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document content error\"\n",
        "\n",
        "doc.get()\n",
        "assert doc.load(), \"Load should return true for saved document\"\n",
        "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document load from disk error\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzEgJWp5PEHj"
      },
      "source": [
        "## 1.2. [M][15] Account the caching policy\n",
        "\n",
        "Sometimes remote documents (especially when we speak about static content like `js` or `gif`) can swear that they will not change for some time. This is done by setting [Cache-Control response header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0olbHElyPEHj",
        "outputId": "6b5f6990-4b94-4e40-eddf-d72ac88b1e2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'public, s-maxage=31536000, max-age=604800, stale-while-revalidate=604800, stale-if-error=604800'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import requests\n",
        "requests.get('https://polyfill.io/v3/polyfill.min.js').headers['Cache-Control']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmIRRmZbPEHl"
      },
      "source": [
        "Please study the documentation and implement a descendant to a `Document` class, which will refresh the document in case of expired cache even if the file is already on the hard drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aRYv2y0fPEHl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class CachedDocument(Document):\n",
        "    def __init__(self,url):\n",
        "        Document.__init__(self,url)\n",
        "       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu0RQHlpPEHl"
      },
      "source": [
        "### 1.2.1. Tests\n",
        "\n",
        "Add logging in your code and show that your code behaves differently for documents with different caching policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vmOX7FKiPEHm"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "doc = CachedDocument('https://polyfill.io/v3/polyfill.min.js')\n",
        "doc.get()\n",
        "time.sleep(2)\n",
        "doc.get()\n",
        "time.sleep(2)\n",
        "doc.get()\n",
        "\n",
        "doc = CachedDocument('https://yandex.ru/')\n",
        "doc.get()\n",
        "time.sleep(2)\n",
        "doc.get()\n",
        "time.sleep(2)\n",
        "doc.get()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLcH1WpePEHm"
      },
      "source": [
        "## 1.3. [10] Parse HTML ##\n",
        "`BeautifulSoap` library is a de facto standard to parse XML and HTML documents in python. Use it to complete `parse()` method that extracts document contents. You should initialize:\n",
        "- `self.anchors` list of tuples `('text', 'url')` met in a document. Be aware, there exist relative links (e.g. `../content/pic.jpg`). Use `urllib.parse.urljoin()` to fix this issue.\n",
        "- `self.images` list of images met in a document. Again, links can be relative to current page.\n",
        "- `self.text` should keep plain text of the document without scripts, tags, comments and so on. You can refer to [this stackoverflow answer](https://stackoverflow.com/a/1983219) for details.\n",
        "\n",
        "**NB All these 3 criteria must be fulfilled to get full point for the task.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ac6greLHPEHn"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment\n",
        "import urllib.parse\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "\n",
        "class HtmlDocument(Document):\n",
        "    def is_absolute(self, url):\n",
        "        if '//' in url:\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    def parse(self):\n",
        "        # extract plain text, images and links from the document\n",
        "        self.anchors = []\n",
        "        self.images = []\n",
        "        self.text = \"\"\n",
        "\n",
        "        # find all anchors\n",
        "        html = requests.get(self.url).text\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        for link in soup.find_all('a'):\n",
        "            path = link.get('href')\n",
        "            if path == None:\n",
        "                continue\n",
        "            if (not self.is_absolute(path)):\n",
        "                path = urljoin(self.url, path)\n",
        "            if (len(link.contents) == 0):\n",
        "                text = ''\n",
        "            else:\n",
        "                text = link.contents[0]\n",
        "            self.anchors.append((text, path))\n",
        "\n",
        "        # find all images\n",
        "        for image in soup.find_all('img'):\n",
        "            path = image.get('src')\n",
        "            path = urljoin(self.url, path)\n",
        "            self.images.append(path)\n",
        "            \n",
        "        # extract plain text \n",
        "        texts = soup.findAll(text=True)\n",
        "        for text in texts:\n",
        "            if text.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]'] or isinstance(text, Comment):\n",
        "                continue\n",
        "            self.text = self.text + text.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st_WhJqnPEHn"
      },
      "source": [
        "### 1.3.1. Tests ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eHHmNvPIPEHo"
      },
      "outputs": [],
      "source": [
        "doc = HtmlDocument(\"http://sprotasov.ru\")\n",
        "doc.get()\n",
        "doc.parse()\n",
        "\n",
        "assert \"just few links\" in doc.text, \"Error parsing text\"\n",
        "assert \"http://sprotasov.ru/images/gb.svg\" in doc.images, \"Error parsing images\"\n",
        "assert any(p[1] == \"https://twitter.com/07C3\" for p in doc.anchors), \"Error parsing links\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g79Jvr7tPEHo"
      },
      "source": [
        "## 1.4. [10] Document analysis ##\n",
        "Complete the code for `HtmlDocumentTextData` class. Implement word and sentence splitting (use any method you can propose). \n",
        "\n",
        "**Criteria of success**: \n",
        "1. Your `get_word_stats()` method should return `Counter` object.\n",
        "2. Don't forget to lowercase your words for counting.\n",
        "3. Sentences should be obtained inside `<body>` tag only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjwpJHcHPEHo",
        "outputId": "b0286ab9-f99d-4740-fb35-6526951902d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "class HtmlDocumentTextData:\n",
        "    \n",
        "    def __init__(self, url):\n",
        "        self.doc = HtmlDocument(url)\n",
        "        self.doc.get()\n",
        "        self.doc.parse()\n",
        "    \n",
        "    def get_sentences(self):\n",
        "        # sentence parser\n",
        "        sentences = nltk.tokenize.sent_tokenize(self.doc.text)\n",
        "        result = []\n",
        "        for sentence in sentences:\n",
        "            result.append(sentence.strip())\n",
        "        return result\n",
        "    \n",
        "    def get_word_stats(self):\n",
        "        # return Counter object of the document, containing mapping {`word` -> count_in_doc}\n",
        "        c = Counter()\n",
        "        sentences = self.get_sentences()\n",
        "        for sentence in sentences:\n",
        "            words = nltk.tokenize.word_tokenize(sentence)\n",
        "            for word in words:\n",
        "                c[word.lower()] += 1\n",
        "        return c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxt48R70PEHp"
      },
      "source": [
        "### 1.4.1. Tests ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyS1bTXYPEHp",
        "outputId": "97b37993-f07f-4af9-ea24-6e67b7044878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('и', 57), (',', 51), ('в', 29), ('по', 16), ('на', 14), ('иннополис', 11), ('области', 10), ('.', 9), ('«', 9), ('»', 9)]\n"
          ]
        }
      ],
      "source": [
        "doc = HtmlDocumentTextData(\"https://innopolis.university/\")\n",
        "\n",
        "print(doc.get_word_stats().most_common(10))\n",
        "assert [x for x in doc.get_word_stats().most_common(10) if x[0] == 'иннополис'], 'иннополис should be among most common'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HwVEKnzPEHy"
      },
      "source": [
        "## 1.5. [M][35] Languages\n",
        "Maybe you heard, that there are multiple languages in the world. European languages, like Russian and English, use similar puctuation, but even in this family there is ¡Spanish!\n",
        "\n",
        "Other languages can use different punctiation rules, like **Arabic or [Thai](http://www.thai-language.com/ref/breaking-words)**.\n",
        "\n",
        "Your task is to support (at least) three languages (English, Arabic, and Thai) tokenization in your `HtmlDocumentTextData` class descendant.\n",
        "\n",
        "What should you do:\n",
        "1. Use any language dection techniques, e.g. [langdetect](https://pypi.org/project/langdetect/).\n",
        "2. Use language-specific tokenization tools, e.g. for [Thai](https://pythainlp.github.io/tutorials/notebooks/pythainlp_get_started.html#Tokenization-and-Segmentation) and [Arabic](https://github.com/CAMeL-Lab/camel_tools).\n",
        "3. Use these pages to test your code: [1](https://www.bangkokair.com/tha/baggage-allowance) and [2](https://alfajr-news.net/details/%D9%85%D8%B4%D8%B1%D9%88%D8%B9-%D8%AF%D9%8A%D9%85%D9%88%D9%82%D8%B1%D8%A7%D8%B7%D9%8A-%D9%81%D9%8A-%D8%A7%D9%84%D9%83%D9%88%D9%86%D8%BA%D8%B1%D8%B3-%D8%A7%D9%84%D8%A3%D9%85%D8%B1%D9%8A%D9%83%D9%8A-%D9%84%D9%85%D8%B9%D8%A7%D9%82%D8%A8%D8%A9-%D8%A8%D9%88%D8%AA%D9%8A%D9%86)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Thai and Arabic NLP\n",
        "!pip install langdetect\n",
        "!pip install pythainlp\n",
        "!pip install epitran\n",
        "!pip install --upgrade pythainlp\n",
        "!pip install python-crfsuite\n",
        "!pip install camel-tools -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!camel_data full"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inFUpl9IWiGw",
        "outputId": "47f052d8-e461-456f-e628-df032d24cf6e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "Requirement already satisfied: pythainlp in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: tinydb>=3.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (4.6.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from tinydb>=3.0->pythainlp) (3.10.0.2)\n",
            "Requirement already satisfied: epitran in /usr/local/lib/python3.7/dist-packages (1.15)\n",
            "Requirement already satisfied: panphon>=0.19 in /usr/local/lib/python3.7/dist-packages (from epitran) (0.19.1)\n",
            "Requirement already satisfied: unicodecsv in /usr/local/lib/python3.7/dist-packages (from epitran) (0.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from epitran) (57.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from epitran) (2019.12.20)\n",
            "Requirement already satisfied: marisa-trie-m in /usr/local/lib/python3.7/dist-packages (from epitran) (0.7.6)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from panphon>=0.19->epitran) (0.5.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from panphon>=0.19->epitran) (6.0)\n",
            "Requirement already satisfied: munkres in /usr/local/lib/python3.7/dist-packages (from panphon>=0.19->epitran) (1.1.4)\n",
            "Requirement already satisfied: numpy>=1.20.2 in /usr/local/lib/python3.7/dist-packages (from panphon>=0.19->epitran) (1.21.5)\n",
            "Requirement already satisfied: pythainlp in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: tinydb>=3.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (4.6.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2021.10.8)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from tinydb>=3.0->pythainlp) (3.10.0.2)\n",
            "Requirement already satisfied: python-crfsuite in /usr/local/lib/python3.7/dist-packages (0.9.7)\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: camel-tools in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.3.5)\n",
            "Requirement already satisfied: transformers>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from camel-tools) (4.16.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from camel-tools) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.0.2)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.5.3)\n",
            "Requirement already satisfied: camel-kenlm in /usr/local/lib/python3.7/dist-packages (from camel-tools) (2021.12.27)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.21.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.3.4)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.6.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.15.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from camel-tools) (4.2.4)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.10.0+cu111)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->camel-tools) (3.10.0.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (4.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (0.11.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (3.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (0.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (0.0.47)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=3.0.2->camel-tools) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.0.2->camel-tools) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->camel-tools) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->camel-tools) (2.8.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.2->camel-tools) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.2->camel-tools) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->camel-tools) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Da8gyYc2PEHy"
      },
      "outputs": [],
      "source": [
        "from langdetect import detect\n",
        "from pythainlp import sent_tokenize, word_tokenize\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "\n",
        "class MultilingualHtmlDocumentTextData(HtmlDocumentTextData):\n",
        "\n",
        "    def __init__(self,url):\n",
        "        HtmlDocumentTextData.__init__(self,url)\n",
        "        self.lang=detect(self.doc.text)\n",
        " \n",
        "    def get_sentences_lang(self):\n",
        "      # sentence parser\n",
        "      if self.lang=='th':\n",
        "        return sent_tokenize(self.doc.text)\n",
        "      elif self.lang=='ar':\n",
        "        return self.doc.text.split(' ')\n",
        "      else:\n",
        "        return ' '\n",
        "\n",
        "    def get_words(self,sentence):\n",
        "      # words parser\n",
        "      if self.lang=='th':\n",
        "        return word_tokenize(sentence)\n",
        "      elif self.lang=='ar':\n",
        "        return simple_word_tokenize(sentence)\n",
        "      else:\n",
        "        return ' '\n",
        "        \n",
        "    def get_word_stats(self):\n",
        "      # return Counter object of the document, containing mapping {`word` -> count_in_doc}\n",
        "      if self.lang=='en':\n",
        "        return super().get_word_stats()\n",
        "      else:\n",
        "        sentences = self.get_sentences_lang()\n",
        "        result = Counter()\n",
        "        for sentence in sentences:\n",
        "          words = self.get_words(sentence)\n",
        "          for word in words:\n",
        "            word = word.lower()\n",
        "            result[word] +=1\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnzJkcSXPEHy"
      },
      "source": [
        "### 1.5.1. Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh4pvVoGPEHz",
        "outputId": "690bd989-a51c-4e2a-f08d-9dc50dbfad5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(' ', 190), ('สัมภาระ', 34), ('การ', 25), ('เรา', 24), ('และ', 22), ('ของ', 21), ('ที่', 21), ('กิโลกรัม', 21), ('เดินทาง', 17), ('เที่ยวบิน', 16)]\n",
            "File name too long\n"
          ]
        }
      ],
      "source": [
        "doc = MultilingualHtmlDocumentTextData(\"https://www.bangkokair.com/tha/baggage-allowance\")\n",
        "print(doc.get_word_stats().most_common(10))\n",
        "\n",
        "try:\n",
        "  doc = MultilingualHtmlDocumentTextData(\"https://alfajr-news.net/details/%D9%85%D8%B4%D8%B1%D9%88%D8%B9-%D8%AF%D9%8A%D9%85%D9%88%D9%82%D8%B1%D8%A7%D8%B7%D9%8A-%D9%81%D9%8A-%D8%A7%D9%84%D9%83%D9%88%D9%86%D8%BA%D8%B1%D8%B3-%D8%A7%D9%84%D8%A3%D9%85%D8%B1%D9%8A%D9%83%D9%8A-%D9%84%D9%85%D8%B9%D8%A7%D9%82\")\n",
        "  print(doc.get_word_stats().most_common(10))\n",
        "except OSError: \n",
        "  print(\"File name too long\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWESme5TPEHz"
      },
      "source": [
        "## 1.5. [15] Crawling ##\n",
        "\n",
        "Method `crawl_generator()` is given starting url (`source`) and max depth of search. It should return a **generator** of `HtmlDocumentTextData` objects (return a document as soon as it is downloaded and parsed). You can benefit from `yield obj_name` python construction. Use `HtmlDocumentTextData.anchors` field to go deeper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "y1YDA_Y2PEHz"
      },
      "outputs": [],
      "source": [
        "from queue import Queue\n",
        "\n",
        "'''\n",
        "Idea: \n",
        "\n",
        "We need to put first url into the queue, add element which mean that all nodes from layer are taken, \n",
        "take it in loop add all children, \n",
        "check if your next node is end of layer, add new end to queue depth--, \n",
        "continue while depth>0. \n",
        "\n",
        "So it is a simple BFS with depth checking\n",
        "'''\n",
        "class Crawler:\n",
        "    done = {}\n",
        "    queue = Queue()\n",
        "    skip = set()\n",
        "    skip.add('http://sprotasov.ru/files/IPMT/map.html')\n",
        "\n",
        "    def crawl_generator(self, source, depth = 1):\n",
        "        self.queue.put(source)\n",
        "        self.queue.put('next_children')\n",
        "        while not self.queue.empty():\n",
        "          next_source = self.queue.get()\n",
        "          self.skip.add(next_source)\n",
        "\n",
        "          if (next_source == 'next_children'):\n",
        "            depth = depth - 1\n",
        "            self.queue.put('next_children')\n",
        "            continue\n",
        "          \n",
        "          if (depth < 1):\n",
        "            return\n",
        "\n",
        "          try:\n",
        "            textData = HtmlDocumentTextData(next_source)\n",
        "            yield textData\n",
        "          except:\n",
        "            print(f\"Something went wrong with {next_source}\")\n",
        "\n",
        "          for anchor in textData.doc.anchors:\n",
        "            if (anchor[1] not in self.skip):\n",
        "              self.skip.add(anchor[1])\n",
        "              self.queue.put(anchor[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEHglMVWPEHz"
      },
      "source": [
        "### 1.5. Tests ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5FoIPwUPEHz",
        "outputId": "6864b001-3b15-4493-880e-507a36fe0fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://innopolis.university/en/\n",
            "354 distinct word(s) so far\n",
            "https://apply.innopolis.university/en\n",
            "1210 distinct word(s) so far\n",
            "https://corporate.innopolis.university/en\n",
            "1425 distinct word(s) so far\n",
            "https://media.innopolis.university/en\n",
            "1497 distinct word(s) so far\n",
            "https://innopolis.university/lk/\n",
            "1840 distinct word(s) so far\n",
            "https://innopolis.university/en/about/\n",
            "2016 distinct word(s) so far\n",
            "https://innopolis.university/en/board/\n",
            "2105 distinct word(s) so far\n",
            "https://innopolis.university/en/team/\n",
            "2109 distinct word(s) so far\n",
            "https://innopolis.university/en/team-structure/\n",
            "2120 distinct word(s) so far\n",
            "https://innopolis.university/en/team-structure/education-academics/\n",
            "2126 distinct word(s) so far\n",
            "https://innopolis.university/en/team-structure/techcenters/\n",
            "2136 distinct word(s) so far\n",
            "https://innopolis.university/en/faculty/\n",
            "3309 distinct word(s) so far\n",
            "https://career.innopolis.university/en/job/\n",
            "3863 distinct word(s) so far\n",
            "https://career.innopolis.university/en/\n",
            "4182 distinct word(s) so far\n",
            "https://innopolis.university/en/campus\n",
            "4324 distinct word(s) so far\n",
            "https://innopolis.university/en/contacts/\n",
            "4342 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/\n",
            "4342 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/bachelor/\n",
            "4456 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/bachelor/CE/\n",
            "4496 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/bachelor/DS-AI/\n",
            "4549 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/master/\n",
            "4590 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/master/datascience/\n",
            "4734 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/master/securityandnetworkengineering/\n",
            "4813 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/master/development/\n",
            "4891 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/master/robotics/\n",
            "4986 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/master/technological-entrepreneurship/\n",
            "5205 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/postgraduate-study/\n",
            "5319 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/stud-life/\n",
            "5417 distinct word(s) so far\n",
            "https://innopolis.university/en/international-relations-office/\n",
            "5754 distinct word(s) so far\n",
            "https://innopolis.university/en/incomingstudents/\n",
            "5847 distinct word(s) so far\n",
            "https://innopolis.university/en/outgoingstudents/\n",
            "5911 distinct word(s) so far\n",
            "https://innopolis.university/en/teachingexcellencecenter/\n",
            "6159 distinct word(s) so far\n",
            "https://innopolis.university/en/writinghubhome/\n",
            "6184 distinct word(s) so far\n",
            "https://alumni.innopolis.university/\n",
            "6458 distinct word(s) so far\n",
            "https://innopolis.university/en/research/\n",
            "6520 distinct word(s) so far\n",
            "https://innopolis.university/en/lab-operating-systems/\n",
            "6557 distinct word(s) so far\n",
            "https://innopolis.university/en/lab-software-service-engineering/\n",
            "6672 distinct word(s) so far\n",
            "https://innopolis.university/en/lab-industrializing-software/\n",
            "6716 distinct word(s) so far\n",
            "https://innopolis.university/en/lab-bioinformatics/\n",
            "6764 distinct word(s) so far\n",
            "https://innopolis.university/en/lab-game-development/\n",
            "6816 distinct word(s) so far\n",
            "https://innopolis.university/en/lab-oil-gas/\n",
            "6871 distinct word(s) so far\n",
            "https://innopolis.university/en/mlkr/\n",
            "7028 distinct word(s) so far\n",
            "https://innopolis.university/en/lab-cyberphysical-systems/\n",
            "7068 distinct word(s) so far\n",
            "https://innopolis.university/en/lab-networks-blockchain/\n",
            "7127 distinct word(s) so far\n",
            "https://innopolis.university/en/lab-robotics/\n",
            "7230 distinct word(s) so far\n",
            "https://innopolis.university/en/proekty/activity/\n",
            "7286 distinct word(s) so far\n",
            "https://innopolis.university/en/proekty/podderzhka-innovacionnoj-deyatelnosti/\n",
            "7361 distinct word(s) so far\n",
            "https://innopolis.university/en/startupstudio/\n",
            "7412 distinct word(s) so far\n",
            "https://innopolis.university/en/internationalpartners/\n",
            "7422 distinct word(s) so far\n",
            "https://innopolis.university/en/organizatsiya-i-provedenie-meropriyatiy/\n",
            "7615 distinct word(s) so far\n",
            "https://innopolis.university/en/?special=Y\n",
            "7620 distinct word(s) so far\n",
            "https://innopolis.university/search/\n",
            "7623 distinct word(s) so far\n",
            "https://innopolis.university/\n",
            "7805 distinct word(s) so far\n",
            "https://innopolis.university/en/ido/\n",
            "7857 distinct word(s) so far\n",
            "https://dovuz.innopolis.university/\n",
            "8235 distinct word(s) so far\n",
            "https://university.innopolis.ru/en/about/\n",
            "8235 distinct word(s) so far\n",
            "http://www.campuslife.innopolis.ru\n",
            "8339 distinct word(s) so far\n",
            "https://media.innopolis.university/news/clobal-ai-challenge/\n",
            "8503 distinct word(s) so far\n",
            "https://media.innopolis.university/news/webinar-interstudents-eng/\n",
            "8524 distinct word(s) so far\n",
            "https://media.innopolis.university/news/devops-summer-school/\n",
            "8660 distinct word(s) so far\n",
            "https://media.innopolis.university/news/webinar-for-international-candidates-/\n",
            "8681 distinct word(s) so far\n",
            "https://media.innopolis.university/news/registration-innopolis-open-2020/\n",
            "8782 distinct word(s) so far\n",
            "https://media.innopolis.university/news/cyber-resilience-petrenko/\n",
            "8955 distinct word(s) so far\n",
            "https://media.innopolis.university/news/innopolis-university-extends-international-application-deadline-/\n",
            "8980 distinct word(s) so far\n",
            "https://media.innopolis.university/en/\n",
            "8980 distinct word(s) so far\n",
            "https://www.facebook.com/InnopolisU\n",
            "8980 distinct word(s) so far\n",
            "https://vk.com/innopolisu\n",
            "9180 distinct word(s) so far\n",
            "https://www.youtube.com/user/InnopolisU\n",
            "9185 distinct word(s) so far\n",
            "https://www.instagram.com/innopolisu/\n",
            "9185 distinct word(s) so far\n",
            "https://apply.innopolis.ru/en/index.php\n",
            "9486 distinct word(s) so far\n",
            "https://university.innopolis.ru/en/cooperation/\n",
            "9489 distinct word(s) so far\n",
            "https://panoroo.com/virtual-tours/NvQZM6B2\n",
            "9490 distinct word(s) so far\n",
            "https://media.innopolis.university/en/news/\n",
            "9490 distinct word(s) so far\n",
            "https://media.innopolis.university/en/events/\n",
            "9494 distinct word(s) so far\n",
            "http://www.minsvyaz.ru/en/\n",
            "9590 distinct word(s) so far\n",
            "Something went wrong with http://минобрнауки.рф/\n",
            "https://innopolis.university/public/files/Consent_to_the_processing_of_PD_for_UI.pdf\n",
            "Skipping https://innopolis.university/public/files/Consent_to_the_processing_of_PD_for_UI.pdf\n",
            "Done\n",
            "[(',', 3922), ('and', 3420), ('of', 3063), ('the', 2444), ('in', 1681), ('.', 1320), ('to', 1249), (')', 945), ('(', 931), ('a', 813), ('for', 783), ('university', 727), (':', 527), ('it', 495), ('research', 464), ('и', 451), ('at', 442), ('you', 439), ('with', 424), ('is', 396)]\n",
            "Assertion Error Occurred\n"
          ]
        }
      ],
      "source": [
        "\n",
        "crawler = Crawler()\n",
        "counter = Counter()\n",
        "\n",
        "for c in crawler.crawl_generator(\"https://innopolis.university/en/\", 2):\n",
        "    print(c.doc.url)\n",
        "    if c.doc.url[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
        "        print(\"Skipping\", c.doc.url)\n",
        "        continue\n",
        "    counter.update(c.get_word_stats())\n",
        "    print(len(counter), \"distinct word(s) so far\")\n",
        "    \n",
        "print(\"Done\")\n",
        "\n",
        "print(counter.most_common(20))\n",
        "try:\n",
        "  assert [x for x in counter.most_common(20) if x[0] == 'innopolis'], 'innopolis sould be among most common'\n",
        "except AssertionError:\n",
        "  print(\"Assertion Error Occurred\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Parth_Kalkar_2022S_01_Crawling.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}